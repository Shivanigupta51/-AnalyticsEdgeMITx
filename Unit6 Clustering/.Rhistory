x <- 5
y <- if(x < 3) {
NA
} else {
10
}
y
g(2)
gf2)
f(2)
f(1)
3L
a=1
b=2
f<-function(x)
{
a*x + b
}
g<-function(x)
{
a=2
b=1
f(x)
}
g(2)
q()
ls()
a
b
cube
cube(b)
ds
str(ds)
setwd('data_R/AnalyticsEdge/Unit6 Clustering/')
flower = read.csv("flower.csv", header=FALSE)
str(flower)
# Change the data type to matrix
flowerMatrix = as.matrix(flower)
str(flowerMatrix)
# Turn matrix into a vector
flowerVector = as.vector(flowerMatrix)
str(flowerVector)
flowerVector2 = as.vector(flower)
str(flowerVector2)
# Compute distances
distance = dist(flowerVector, method = "euclidean")
str(distance)
str(flower)
str(flowerMatrix)
plot(flowerMatrix)
# Hierarchical clustering
clusterIntensity = hclust(distance, method="ward")
plot(clusterIntensity)
rect.hclust(clusterIntensity, k = 3, border = "red")
flowerClusters = cutree(clusterIntensity, k = 3)
flowerClusters
# Find mean intensity values
tapply(flowerVector, flowerClusters, mean)
# Plot the image and the clusters
dim(flowerClusters) = c(50,50)
image(flowerClusters, axes = FALSE)
# Original image
image(flowerMatrix,axes=FALSE,col=grey(seq(0,1,length=256)))
image(flowerMatrix)
image(flowerMatrix)
image(flowerMatrix)
image(flowerClusters, axes = FALSE)
image(flowerMatrix)
image(flowerClusters, axes = FALSE)
image(flowerMatrix,axes=FALSE,col=grey(seq(0,1,length=256)))
rm(list=ls())
files.list()
list.files()
healthy = read.csv("healthy.csv", header=FALSE)
healthyMatrix = as.matrix(healthy)
str(healthyMatrix)
# Plot image
image(healthyMatrix,axes=FALSE,col=grey(seq(0,1,length=256)))
image(healthyMatrix,col=grey(seq(0,1,length=256)))
# Hierarchial clustering
healthyVector = as.vector(healthyMatrix)
distance = dist(healthyVector, method = "euclidean")
# We have an error - why?
str(healthyVector)
# Video 5
# Specify number of clusters
k = 5
# Run k-means
set.seed(1)
KMC = kmeans(healthyVector, centers = k, iter.max = 1000)
str(KMC)
# Extract clusters
healthyClusters = KMC$cluster
KMC$centers[2]
KMC = kmeans(healthyVector, centers = k, iter.max = 10000)
KMC = kmeans(healthyVector, centers = k)
KMC
KMC = kmeans(healthyVector, centers = k, iter.max = 10000)
KMC
KMC = kmeans(healthyVector, centers = k, iter.max = 1000)
KMC
str(KMC)
KMC$centers
# Plot the image with the clusters
dim(healthyClusters) = c(nrow(healthyMatrix), ncol(healthyMatrix))
image(healthyClusters, axes = FALSE, col=rainbow(k))
image(healthyClusters)
image(healthyClusters, axes = FALSE, col=rainbow(k))
image(healthyClusters, axes = FALSE, col=rainbow(k))
rainbow(k)
rainbow(k)
rainbow(k)
rainbow(k)
image(healthyClusters, axes = FALSE, col=rainbow(k))
image(healthyClusters, axes = FALSE, col=rainbow(k))
image(healthyClusters, axes = FALSE, col=rainbow(k))
image(healthyClusters, axes = FALSE, col=rainbow(k))
tumor = read.csv("tumor.csv", header=FALSE)
tumorMatrix = as.matrix(tumor)
tumorVector = as.vector(tumorMatrix)
# Apply clusters from before to new image, using the flexclust package
install.packages("flexclust")
library(flexclust)
KMC.kcca = as.kcca(KMC, healthyVector)
tumorClusters = predict(KMC.kcca, newdata = tumorVector)
# Visualize the clusters
dim(tumorClusters) = c(nrow(tumorMatrix), ncol(tumorMatrix))
image(tumorClusters, axes = FALSE, col=rainbow(k))
image(tumorMatrix)
image(tumorClusters, axes = FALSE, col=rainbow(k))
image(tumorClusters, axes = FALSE, col=rainbow(k))
image(tumorMatrix)
image(tumorClusters, axes = FALSE, col=rainbow(k))
rm(list=ls())
gc()
gc()
getwd()
setwd('../../churn')
files.list()
list.files()
dat<-read.csv('churn.cbc.csv', sep=";");
# full anonymization
dat$name<-NULL;
dat$ort<-NULL;
dat0<-(subset(dat, calls_total>10))
tree()
library(tree)
rpart()
library(rpart)(
library(rpart)
str(dat0)
model.CART = rpart(churn~., data=dat0)
model.CART
rpart(churn~., data=dat0, cp=0.1)
rpart(churn~., data=dat0, cp=0.01)
rpart(churn~., data=dat0, cp=10)
rpart(churn~., data=dat0, cp=0.001)
rpart(churn~., data=dat0, cp=0.00001)
rpart(churn~., data=dat0, cp=0.0000001)
?rpart
rpart(churn~., data=dat0, control = rpart.control(cp = 0.05))
plot(model.CART); text(model.CART)
library(randomForest)
churn.train.rf<-randomForest(churn~., data=dat[train, -1], ntree=100, importance=T)
churn.train.rf<-randomForest(churn~., data=dat0, ntree=100, importance=T)
churn.train.rf
varImpPlot(churn.train.rf)
prp
library(rpart.control)
library(rpart.plot)
prp(model.CART)
churn.train.rf
summary(churn.train.rf)
importance(churn.train.rf)
importance(churn.train.rf)[,4]
sort(importance(churn.train.rf)[,4])
importance(churn.train.rf)
rpart(churn~., data=dat0, control = rpart.control(cp = 0.00001))
rpart(churn~., data=dat0, control = rpart.control(cp = 0.000000001))
rpart(churn~., data=dat0, control = rpart.control(cp = 0.01))
varImpPlot(churn.train.rf)
prune.tree
getwd()
rm(list=ls()); gc()
setwd('../AnalyticsEdge/Unit6 Clustering/')
dat <- read.csv('dailykos.csv');
str(dat)
dim(dat)
edit(dat)
dim(dat)
dat[1,]
table(dat[1,])
names(dat)
names(dat)[1:10]
dim(dat)
image(as.matrix(dat))
dim(dat)
str(dat)
# Compute distances
distances = dist(dat, method = "euclidean")
str(distances)
kosDist = distances
kosHierClust = hclust(kosDist, method="ward.D")
kosHierClust
summary(kosHierClust)
plot(kosHierClust)
kosGroups = cutree(kosHierClust, k = 7)
kosGroups
table(kosGroups)
table(kosGroups)kosGroups[1]
kosGroups[1]
kosGroups[1,]
str(kosGroups)
kosCluster1 = subset(dailykos, kosGroups == 1)
kosCluster2 = subset(dailykos, kosGroups == 2)
kosCluster3 = subset(dailykos, kosGroups == 3)
kosCluster4 = subset(dailykos, kosGroups == 4)
kosCluster5 = subset(dailykos, kosGroups == 5)
kosCluster6 = subset(dailykos, kosGroups == 6)
kosCluster7 = subset(dailykos, kosGroups == 7)
table(kosGroups)
subset(dailykos, kosGroups == 1)
dailykos
dailykos  <- read.csv('dailykos.csv');
ls()
kosCluster1 = subset(dailykos, kosGroups == 1)
kosCluster2 = subset(dailykos, kosGroups == 2)
kosCluster3 = subset(dailykos, kosGroups == 3)
kosCluster4 = subset(dailykos, kosGroups == 4)
kosCluster5 = subset(dailykos, kosGroups == 5)
kosCluster6 = subset(dailykos, kosGroups == 6)
kosCluster7 = subset(dailykos, kosGroups == 7)
kosCluster1
colsum(kosCluster1)
colSum(kosCluster1)
colSums(kosCluster1)
sort(colSums(kosCluster1))
tail(sort(colSums(kosCluster1)))
tail(sort(colSums(kosCluster2)))
tail(sort(colSums(kosCluster3)))
tail(sort(colSums(kosCluster4)))
tail(sort(colSums(kosCluster5)))
tail(sort(colSums(kosCluster6)))
tail(sort(colSums(kosCluster7)))
kosClusters = split(dailykos, kosGroups)
kosClusters[1]
tail(sort(colSums(kosCluster7)))
tail(sort(colSums(kosClusters)))
tail(sort(colSums(kosClusters[1:7])))
tail(sort(colSums(kosClusters[1])))
tail(sort(colSums(kosClusters[1])))
tail(sort(colSums(kosClusters[[1]])))
tail(sort(colSums(kosClusters[[1:10]])))
for(i in 1:10) tail(sort(colSums(kosClusters[[i]])))
for(i in 1:7) tail(sort(colSums(kosClusters[[i]])))
for(i in 1:7) (tail(sort(colSums(kosClusters[[i]]))))
for(i in 1:7) print(tail(sort(colSums(kosClusters[[i]]))))
for(i in 1:7) { print(i); print("\n"); print(tail(sort(colSums(kosClusters[[i]])))); print(\n");}
for(i in 1:7) { print(paste(i, tail(sort(colSums(kosClusters[[i]]))}
for(i in 1:7) { print(paste(i, tail(sort(colSums(kosClusters[[i]])))}
for(i in 1:7) { print(paste(i, tail(sort(colSums(kosClusters[[i]]))))}
for(i in 1:7) { print(paste(i, tail(sort(colSums(kosClusters[[i]])))))}
for(i in 1:7) { print(paste(i, tail(sort(colMeans(kosClusters[[i]])))))}
for(i in 1:7) print(tail(sort(colSums(kosClusters[[i]]))))
for(i in 1:7) print(tail(sort(colMeans(kosClusters[[i]]))))
kosClusters
set.seed(1000);
KMC = kmeans(kosDist, centers = k)
str(KMC)
k
k=7
KMC = kmeans(kosDist, centers = k)
KMC
table(KMC)
str(KMC)
table(KMC$cluster)
set.seed(1000);
KMC = kmeans(kosDist, centers = k)
table(KMC$cluster)
(KMC$cluster)
kosKClusters = split(dailykos, KMC$cluster)
kosKClusters[1]
colMeans(kosKClusters[1])
kosKClusters[1]
colSums(kosKClusters[1])
str(kosKClusters[1])
colSums(as.matrix(kosKClusters[1]))
str(as.matrix(kosKClusters[1]))
k
set.seed(1000);
KMC = kmeans(kosDist, centers = k)
table(KMC$cluster)
kosKmeanCluster1 = subset(dailykos, KMC$cluster == 1)
str(kosKmeanCluster1)
summary(kosKmeanCluster1)
colMeans(kosKmeanCluster1)
set.seed(1000);
KMC = kmeans(dailykos, centers = k)
table(KMC$cluster)
kosKClusters = split(dailykos, KMC$cluster)
str(kosKClusters)
(kosKClusters[1])
colMeans(kosKClusters[1])
kosKmeanCluster1 = subset(dailykos, KMC$cluster == 1)
kosKmeanCluster2 = subset(dailykos, KMC$cluster == 2)
kosKmeanCluster3 = subset(dailykos, KMC$cluster == 3)
kosKmeanCluster4 = subset(dailykos, KMC$cluster == 4)
kosKmeanCluster5 = subset(dailykos, KMC$cluster == 5)
kosKmeanCluster6 = subset(dailykos, KMC$cluster == 6)
kosKmeanCluster7 = subset(dailykos, KMC$cluster == 7)
colMeans(KMC$cluster)
colMeans(kosKmeanCluster1)
tail(sort(colMeans(kosKmeanCluster1)))
colMeans(kosKClusters[[1]])
colMeans(kosKClusters[[1]])
tail(sort(colMeans(kosKClusters[[1]])))
for (i in 1:k) (sort(colMeans(kosKClusters[[i]])))
for (i in 1:k) tail(sort(colMeans(kosKClusters[[i]])))
k
for (i in 1:k) (tail(sort(colMeans(kosKClusters[[i]]))))
tail(sort(colMeans(kosKClusters[[1]])))
tail(sort(colMeans(kosKClusters[[2]])))
for (i in 1:k) print(tail(sort(colMeans(kosKClusters[[i]]))))
print("K-MEANS CLUSTERING")
for (i in 1:k) print(tail(sort(colMeans(kosKClusters[[i]]))))
# top words by cluster for K-means
print("K-MEANS CLUSTERING")
for (i in 1:k) print(tail(sort(colMeans(kosKClusters[[i]]))))
print("HIERARCHICAL CLUSTERING")
for(i in 1:7) print(tail(sort(colSums(kosClusters[[i]]))))
for(i in 1:7) print(tail(sort(colMeans(kosClusters[[i]]))))
# top words by cluster for hierarchical
print("HIERARCHICAL CLUSTERING")
for(i in 1:7) print(tail(sort(colMeans(kosClusters[[i]]))))
# top words by cluster for K-means
print("K-MEANS CLUSTERING")
for (i in 1:k) print(tail(sort(colMeans(kosKClusters[[i]]))))
table(hierGroups, KmeansCluster$cluster)
table(kosGroups, KmeansCluster$cluster)
table(kosGroups, KMC$cluster)
rm(list=ls())
gc()
airlines = read.csv('AirlinesCluster.csv');
str(airlinse)
str(airlines)
plot(airlines)
summary(airlines)
boxplot(airlines)
library(caret)
# The first command pre-processes the data, and the second command performs the normalization. If you look at the summary of airlinesNorm, you should see that all of the variables now have mean zero. You can also see that each of the variables has standard deviation 1 by using the sd() function.
preproc = preProcess(airlines)
airlinesNorm = predict(preproc, airlines)
install.packages(caret)
install.packages('caret')
library(caret)
preproc = preProcess(airlines)
airlinesNorm = predict(preproc, airlines)
summary(airlinesNorm)
summary(airlinesNorm)
sd(airlinesNorm)
sd(airlinesNorm$Balance)
summary(airlinesNorm$Balance)
sd(airlinesNorm$Balance)
var(airlinesNorm$Balance)
plot(airlinesNorm)
# Compute distances
HierAirlines = dist(airlinesNorm , method = "euclidean")
# hierarchical clustering  
HierClust = hclust(HierAirlines, method="ward.D")
# Plot the dendrogram
plot(HierClust)
kosGroups = cutree(kosHierClust, k = 5)
# counts in each cluster
table(kosGroups)
HierGroups = cutree(HierClust, k = 5)
# counts in each cluster
table(HierGroups)
tapply(airlines$Balance, HierGroups, mean)
tapply(airlines, HierGroups, mean)
lapply(airlines, HierGroups, mean)
HierGroups
lapply(airlines, HierGroups, mean)
tapply(airlines$Balance, HierGroups, mean)
str(airlines)
ncol(airlines)
for(i in 1:ncol(airlines)) tapply(airlines[,i], HierGroups, mean)
for(i in 1:ncol(airlines)) print(tapply(airlines[,i], HierGroups, mean))
for (i in names(airlines)) print i
for (i in names(airlines)) print(i)
for(var in names(airlines)) tapply(airlines[,var], HierGroups, mean)
for(var in names(airlines)) print(tapply(airlines[,var], HierGroups, mean))
for(var in names(airlines)) print(var); print(tapply(airlines[,var], HierGroups, mean))
for(var in names(airlines)){print(var); print(tapply(airlines[,var], HierGroups, mean))}
table(HierGroups)
subset(airlines, HierGroups ==1)
summary(subset(airlines, HierGroups ==1))
summary(subset(airlines, HierGroups ==2))
summary(subset(airlines, HierGroups ==3))
summary(subset(airlines, HierGroups ==4))
summary(subset(airlines, HierGroups ==5))
lapply(split(airlines, clusterGroups), colMeans)
lapply(split(airlines, HierGroups), colMeans)
round(lapply(split(airlines, HierGroups), colMeans),2)
lapply(split(airlines, HierGroups), colMeans),2
lapply(split(airlines, HierGroups), colMeans)
lapply(split(airlines, HierGroups), colMeans, 2)
lapply(split(airlines, HierGroups), colMeans)
lapply(split(airlines, HierGroups), round(colMeans,2))
lapply(split(airlines, HierGroups), round(colMeans))
lapply(split(airlines, HierGroups), colMeans)
tapply(split(airlines, HierGroups), colMeans)
apply(split(airlines, HierGroups), colMeans)
apply(split(airlines, HierGroups), FUN=colMeans)
options(digits=2)
tapply(split(airlines, HierGroups), colMeans)
lapply(split(airlines, HierGroups), colMeans)
options(digits=4)
lapply(split(airlines, HierGroups), colMeans)
options(digits=10)
lapply(split(airlines, HierGroups), colMeans)
options(digits=6)
lapply(split(airlines, HierGroups), colMeans)
plot(lapply(split(airlines, HierGroups), colMeans))
m<-(lapply(split(airlines, HierGroups), colMeans))
m
round(m, 2)
str(m)
as.matrix(m)
as.matrix(m)
(m)
(m$1)
m[[1]]
m[[2]]
round(m[[1]],3)
round(m[[1:5]],3)
for(i in 1:5) print(round(m[[i]],3))
for(i in 1:5) print(round(m[[i]],3))
KMC = kmeans(airlines, centers = k, iter.max = 1000)
k
k=5
KMC = kmeans(airlines, centers = k, iter.max = 1000)
KMC
table(KMC$cluster)
kmeansClust$centers
kmeansClust = KMC
kmeansClust$centers
kmeansClust = kmeans(airlinesNorm, centers = k, iter.max = 1000)
table(kmeansClust$cluster)
kmeansClust = kmeans(airlinesNorm, centers = k, iter.max = 1000)
set.seed(88);
table(kmeansClust$cluster)
set.seed(88);
kmeansClust = kmeans(airlinesNorm, centers = k, iter.max = 1000)
table(kmeansClust$cluster)
set.seed(88);
kmeansClust = kmeans(airlinesNorm, centers = k, iter.max = 1000)
table(kmeansClust$cluster)
kmeansClust$centers
rm(list=ls())
gc()
stocks = read.csv('StocksCluster.csv');
str(stocks)
plot(stocks)
boxplot(stocks)
prop.table(table(stocks$PositiveDec))
cor(stocks)
sort(cor(stocks))
cor(stocks)
colMeans(stocks)
summary(stocks)
means(stocks)
mean(stocks)
set.seed(144)
spl = sample.split(stocks$PositiveDec, SplitRatio = 0.7)
stocksTrain = subset(stocks, spl == TRUE)
stocksTest = subset(stocks, spl == FALSE)
library(caTools)
set.seed(144)
spl = sample.split(stocks$PositiveDec, SplitRatio = 0.7)
stocksTrain = subset(stocks, spl == TRUE)
stocksTest = subset(stocks, spl == FALSE)
StocksModel = glm(PositiveDec ~ ., data=stocksTrain, family="binomial")
summary(StocksModel)
table(
table(stocksTrain, predict(StocksModel))
table(stocksTrain, predict(StocksModel)>0.5)
stocksTrain
stocksTrain$PositiveDec
table(stocksTrain$PositiveDec, predict(StocksModel)>0.5)
(3324+737)/nrow(stocksTrain)
nrow(stocksTrain)
3324+3690+355+737
library(caTools)
set.seed(144)
spl = sample.split(stocks$PositiveDec, SplitRatio = 0.7)
stocksTrain = subset(stocks, spl == TRUE)
stocksTest = subset(stocks, spl == FALSE)
# train a logistic regression model 
StocksModel = glm(PositiveDec ~ ., data=stocksTrain, family="binomial")
# overall accuracy on the training set, using a threshold of 0.5
table(stocksTrain$PositiveDec, predict(StocksModel)>0.5)
table(stocksTest$PositiveDec, predict(StocksModel, newdata=stocksTest)>0.5)
(1427+312)/nrow(stocksTest)
dim(stocksTrain)
summary(StocksModel)
(StocksModel)
str(stocks)
table(stocksTest$PositiveDec, predict(StocksModel, newdata=stocksTest)>0.5)
StocksModel = glm(PositiveDec ~ ., data=stocksTrain, family=binomial)
table(stocksTrain$PositiveDec, predict(StocksModel)>0.5)
predict(StocksModel)
predict(StocksModel)[,2]
